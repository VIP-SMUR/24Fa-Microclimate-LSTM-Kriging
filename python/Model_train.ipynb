{"cells":[{"cell_type":"markdown","metadata":{"id":"Of9Zo8IG6acm"},"source":["\n","# Setup\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1695980001182,"user":{"displayName":"Joie Lim","userId":"08814076690383135501"},"user_tz":-480},"id":"OQ5-Jyps6acj","jupyter":{"outputs_hidden":false},"tags":[]},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4824,"status":"ok","timestamp":1695980028233,"user":{"displayName":"Joie Lim","userId":"08814076690383135501"},"user_tz":-480},"id":"f-UiUy8B6aco","outputId":"d02b4ca5-b50e-42c3-91d5-83c635347fe2","tags":[]},"outputs":[],"source":["import sys\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import os\n","import json\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVR\n","from datetime import date\n","from pvlib.iotools import read_epw\n","\n","from pykrige.rk import RegressionKriging\n","from pykrige.ok import OrdinaryKriging\n","import pickle"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1695980036519,"user":{"displayName":"Joie Lim","userId":"08814076690383135501"},"user_tz":-480},"id":"Ds8JsapSAH8r"},"outputs":[],"source":["dir = 'C:\\\\GitHub\\\\microclimate-dl-predict\\\\data\\\\'"]},{"cell_type":"markdown","metadata":{"id":"WUmUksbS8guq"},"source":["\n","# Load, verify and process input files"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["measurenames={\"RH0719\":'RH',\"Tem0719\":'Temperature'}\n","measureunit={\"Tem0719\":'Â°C',\"RH0719\":'%'}\n","measure = \"RH0719\"\n","#measure = \"Tem0719\"\n","testmeasure = \"RH0819\"\n","#testmeasure = \"Tem0819\"\n","df = pd.read_csv(dir + measure +'.csv')\n","testdf = pd.read_csv(dir + testmeasure +'.csv')\n","\n","\n","# df = df.drop(df.index[2])\n","\n","amy,meta=read_epw(dir+'SGP_Singapore.486980_IWEC.epw')\n","changi,meta=read_epw(dir+'SGP_SINGAPORE-CHANGI-AP_486980S_19.epw')\n","if measure == \"Tem0719\":\n","    mykey='temp_air'\n","else:\n","    mykey='relative_humidity'\n","changidata=changi.loc[(changi['year'].astype('str')+changi['month'].astype('str')=='20197'),[mykey]]\n","amydata=amy.loc[(amy['month'].astype('str')=='7'),[mykey]]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["windspeeddf = pd.read_csv(dir + 'ws0719.csv')\n","solardf = pd.read_csv(dir + 'sr0719.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":866},"executionInfo":{"elapsed":3796,"status":"ok","timestamp":1695984815898,"user":{"displayName":"Joie Lim","userId":"08814076690383135501"},"user_tz":-480},"id":"VFAhPxd66acp","outputId":"c1bba5e9-e61b-4e12-cc2e-ba12c0122285"},"outputs":[],"source":["griddf = pd.read_csv(dir + '1m_GridPoints_distTo_and_zones_3414.csv').fillna(0)\n","wsdf = pd.read_csv(dir + 'WSPoints_TreesAndStreets_3414.csv').fillna(0)\n","t = pd.concat([griddf, pd.get_dummies(griddf.HorticultureZone)], axis=1)\n","t = t.replace(\"Temparea\", 1)\n","t = t.replace(\"Vegetated Ridge\", 1)\n","# t = t.replace(\"Valley\", 1)\n","# t = t.rename(columns={\"Area\": \"VegetatedRidge\"})\n","### drop features that share high correlation with other features (not useful for prediction)\n","griddf = t.drop(['id','HorticultureZone'], axis=1)\n","# griddf = t.drop(['id','HorticultureZone','gridarea','buildingfootprintarea','roadarea','patharea','walkwayarea','courttrackarea','carparkarea'], axis=1)\n","### set points within VegetatedRidge to have distToTree 0.0 (trees within the area are not doccumented but the area is densely forrested)\n","### helps a bit to make distToTree useful but numtrees is still inaccurate for this area\n","griddf['distToTree'].loc[griddf['VegetatedRidge'] == 1] = 0.0\n","\n","t = pd.concat([wsdf, pd.get_dummies(wsdf.HorticultureZone)], axis=1)\n","t = t.replace(\"Temparea\", 1)\n","t = t.replace(\"Vegetated Ridge\", 1)\n","t = t.replace(\"Valley\", 1)\n","# t = t.rename(columns={\"Area\": \"VegetatedRidge\"})\n","t = t.sort_values(by=['ID'])\n","### drop features that share high correlation with other features (not useful for prediction)\n","wsdf = t.drop(['Lat','Long','HorticultureZone','Location','Type','Description','gridarea','buildingfootprintarea','roadarea','patharea','walkwayarea','courttrackarea','carparkarea'], axis=1)\n","wsdf = wsdf.reset_index(drop=True)\n","\n","plotdf = testdf.transpose().iloc[2:16,:]\n","plotdf['X']=list(wsdf['X'])\n","plotdf['Y']=list(wsdf['Y'])\n","\n","display(wsdf)\n","display(griddf.head())"]},{"cell_type":"markdown","metadata":{"id":"UZf8juVc85Hy"},"source":["# Regression kriging"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["allstations=np.array(range(14))\n","targetstationid=7\n","otherstationsid=np.delete(allstations,targetstationid).tolist()"]},{"cell_type":"code","execution_count":391,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","from torch.autograd import Variable\n","\n","percentfeatures=['percentroad','percentpath','percentwalkway','percentcourttrack','percentcarpark']\n","\n","class MyModel(nn.Module):\n","    def __init__(self, embedding_length, hidden_size,output_size,batch_size):\n","        self.hidden_size = hidden_size\n","        self.batch_size = batch_size\n","        super(MyModel, self).__init__()\n","        self.lstm = nn.LSTM(embedding_length, hidden_size,batch_first=True)\n","        self.label = nn.Linear(hidden_size, output_size)\n","    def forward(self, input):\n","        h_0 = Variable(torch.zeros(1,self.batch_size, self.hidden_size))\n","        c_0 = Variable(torch.zeros(1,self.batch_size, self.hidden_size))\n","        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n","        return self.label(final_hidden_state[-1]) \n","class gru(nn.Module):\n","    def __init__(self, embedding_length, hidden_size,output_size,batch_size):\n","        self.hidden_size = hidden_size\n","        self.batch_size = batch_size\n","        super(gru, self).__init__()\n","        self.lstm = nn.GRU(embedding_length, hidden_size,batch_first=True)\n","        self.label = nn.Linear(hidden_size, output_size)\n","    def forward(self, input):\n","        h_0 = Variable(torch.zeros(1,self.batch_size, self.hidden_size))\n","        \n","        output, final_hidden_state = self.lstm(input, h_0)\n","        return self.label(final_hidden_state[-1]) \n","\n","embedding_length=15\n","batchsize=16\n","mymodel=gru(embedding_length=embedding_length,hidden_size=64,output_size=13,batch_size=batchsize)\n","optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.01, momentum=0.9)\n","\n","def train_model(model, train_iter, epoch):\n","    model.train()\n","    for e in range(epoch):\n","        loss=0\n","        optimizer.zero_grad()\n","        for idx, batch in enumerate(train_iter):\n","            prediction = model(batch.x)\n","            loss = loss + torch.norm(prediction-batch.y,\"fro\")\n","        loss=loss/(idx+1)\n","        print(\"epoch {}: Training Loss normalized Root MSE : {} %\".format(e,np.sqrt(loss.detach().numpy())) )\n","        loss.backward()\n","        optimizer.step()\n","        if loss <0.1:\n","            break\n","    return loss\n","\n","class Mydata():\n","    def __init__(self,x,y):\n","        self.x=x\n","        self.y=y\n","\n","measure='Tem0719'\n","npdata=np.array(pd.read_csv(dir + measure +'.csv').iloc[:,2:16])\n","hourdata=np.sum(npdata.reshape([-1,60,14]),axis=1)/60\n","testhourdata=np.sum(np.array(testdf.iloc[:,2:16]).reshape([-1,60,14]),axis=1)/60\n","mean = hourdata.mean()\n","std = hourdata.std()\n","pdhourdata=pd.DataFrame(hourdata).transpose()\n","pdhourdata['X']=list(wsdf['X'])\n","pdhourdata['Y']=list(wsdf['Y'])\n","\n","    "]},{"cell_type":"code","execution_count":762,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Date : 05/07/19, Time : 04:00:00\n","========================================\n","Finished learning regression model\n","Finished kriging residuals\n","Date : 09/07/19, Time : 20:00:00\n","========================================\n","Finished learning regression model\n","Finished kriging residuals\n","Date : 22/07/19, Time : 12:00:00\n","========================================\n","Finished learning regression model\n","Finished kriging residuals\n"]}],"source":["measure='Tem0719'\n","npdata=np.array(pd.read_csv(dir + measure +'.csv').iloc[:,2:16])\n","hourdata=np.sum(npdata.reshape([-1,60,14]),axis=1)/60\n","testhourdata=np.sum(np.array(testdf.iloc[:,2:16]).reshape([-1,60,14]),axis=1)/60\n","mean = hourdata.mean()\n","std = hourdata.std()\n","\n","data=[]\n","testdata=[]\n","allstations=np.array(range(14))\n","targetstationid=7\n","otherstationsid=np.delete(allstations,targetstationid).tolist()\n","with open('trainednw\\\\{}lstmmodel_target{}.pkl'.format(measure,targetstationid), 'rb') as f:\n","        mymodel = pickle.load(f)\n","if batchsize ==1:\n","    for i in range(0,hourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        data.append(Mydata(torch.tensor((np.delete(hourdata[i:i+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32).transpose(0,1),torch.tensor((np.delete(hourdata[i+embedding_length:i+1+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","    for i in range(0,testhourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        testdata.append(Mydata(torch.tensor((np.delete(testhourdata[i:i+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32).transpose(0,1),torch.tensor((np.delete(testhourdata[i+embedding_length:i+1+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","\n","else:\n","    for i in range(0,hourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        data.append(Mydata(torch.tensor(np.array([(np.delete(hourdata[i+k:i+k+embedding_length,:],targetstationid,axis=1)-mean)/std for k in range(batchsize)]),dtype=torch.float32).transpose(1,2)\n","                    ,torch.tensor((np.delete(hourdata[i+embedding_length:i+batchsize+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","    for i in range(0,testhourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        testdata.append(Mydata(torch.tensor(np.array([(np.delete(testhourdata[i+k:i+k+embedding_length,:],targetstationid,axis=1)-mean)/std for k in range(batchsize)]),dtype=torch.float32).transpose(1,2)\n","                    ,torch.tensor((np.delete(testhourdata[i+embedding_length:i+batchsize+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","\n","griddflist=[]\n","plotdflist=[]\n","for i, mth in enumerate(months):\n","    griddflist.append(griddf.copy())\n","    plotdflist.append(plotdf.copy())\n","    print(\"Date : {}, Time : {}\".format(df.iloc[(mth*batchsize+batchindex+embedding_length)*60,0],df.iloc[(mth*batchsize+batchindex+embedding_length)*60,1]))\n","\n","    x = np.array(list(zip(wsdf.drop(targetstationid).X, wsdf.drop(targetstationid).Y)))\n","    truevalue = hourdata[mth*batchsize+batchindex+embedding_length,:]\n","    lastdayvalue = hourdata[mth*batchsize+batchindex+embedding_length-24,:]\n","    target = mymodel(data[mth].x)[batchindex,:].detach().numpy()*std+mean\n","    # p_train, p_test, x_train, x_test, target_train, target_test = train_test_split(\n","    #     p, x, target, test_size=0.3, random_state=42\n","    # )\n","    plotdf['plotdata']=truevalue\n","    print(\"=\" * 40)\n","    m_ok = OrdinaryKriging(x[:,0],x[:,1],target,variogram_model=k,verbose=False)\n","    m_rk = RegressionKriging(regression_model=model, n_closest_points=n, variogram_model=k, verbose=False)\n","\n","    # m_rk.fit(p_train, x_train,target_train)\n","    # score = m_rk.score(p_test, x_test,target_test)\n","    # scores_mths.append(score)\n","\n","    m_rk.fit(p, x, target)\n","    result = m_rk.predict(target_p, target_x)\n","    griddflist[i][measure+'_lstmrk_'] = result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["error=[]\n","for k in variogram_models:\n","    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n","    for mth in range(len(data)):\n","        for batchindex in range(batchsize):\n","            truevalue = hourdata[mth*batchsize+batchindex+embedding_length,targetstationid]\n","            target = mymodel(data[mth].x)[batchindex,:].detach().numpy()*std+mean\n","            m_rk = RegressionKriging(regression_model=model, n_closest_points=n, variogram_model=k, verbose=False)\n","            m_rk.fit(p, x, target)\n","            result = m_rk.predict(target_p, target_x)\n","            plotdf.iloc[[targetstationid],embedding_length+mth*batchsize+batchindex]=(result-truevalue)"]},{"cell_type":"markdown","metadata":{},"source":["# Model training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Select features for model, excluded most of the percent** because they seem quite small and not significant, especially for features with small area like walkways\n","features = ['terrain','distToBuilding','distToTree','distToWalkway','distToRoad','distToPath','distToCourtTrack','distToCarpark']\n","p = wsdf[features].drop(targetstationid)\n","target_p = wsdf.loc[targetstationid,features].to_numpy().reshape(1,-1)\n","target_x = np.array([(wsdf.loc[targetstationid,'X'],wsdf.loc[targetstationid,'Y'])]).reshape(1,-1)\n","\n","feature_importance_df = pd.DataFrame(features, columns =['FeatureName'])\n","\n","## prepare data for training and models to train\n","data=[]\n","testdata=[]\n","if batchsize ==1:\n","    for i in range(0,hourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        data.append(Mydata(torch.tensor((np.delete(hourdata[i:i+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32).transpose(0,1),torch.tensor((np.delete(hourdata[i+embedding_length:i+1+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","    for i in range(0,testhourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        testdata.append(Mydata(torch.tensor((np.delete(testhourdata[i:i+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32).transpose(0,1),torch.tensor((np.delete(testhourdata[i+embedding_length:i+1+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","\n","else:\n","    for i in range(0,hourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        data.append(Mydata(torch.tensor(np.array([(np.delete(hourdata[i+k:i+k+embedding_length,:],targetstationid,axis=1)-mean)/std for k in range(batchsize)]),dtype=torch.float32).transpose(1,2)\n","                    ,torch.tensor((np.delete(hourdata[i+embedding_length:i+batchsize+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","    for i in range(0,testhourdata.shape[0]-embedding_length-batchsize,batchsize):\n","        testdata.append(Mydata(torch.tensor(np.array([(np.delete(testhourdata[i+k:i+k+embedding_length,:],targetstationid,axis=1)-mean)/std for k in range(batchsize)]),dtype=torch.float32).transpose(1,2)\n","                    ,torch.tensor((np.delete(testhourdata[i+embedding_length:i+batchsize+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","\n","\n","methods=['_lstmrk_','_lstmok_','_grurk_','_gruok_','_ne_','_iwec_','_changi_']#'\n","puredata={}\n","for method in methods:\n","    puredata[measure+method]=plotdf.iloc[:,embedding_length:embedding_length+len(data)*batchsize].to_numpy()\n","### SVR model parameters\n","C = 0.0005\n","gamma = 5\n","kernel = ['linear'] # options: ['linear', 'poly', 'rbf', 'sigmoid']\n","\n","### RandomForestRegressor parameters\n","n_estimators=50\n","random_state=4\n","\n","### RegressionKrigging parameters\n","n = 8\n","variogram_models = ['spherical'] # options: [\"linear\", \"power\", \"gaussian\", \"spherical\", \"exponential\"]\n","\n","#months = [\"Feb-19\",\"Mar-19\",\"Apr-19\",\"May-19\",\"Jun-19\",\"Jul-19\",\"Aug-19\",\"Sep-19\",\"Oct-19\",\"Nov-19\",\"Dec-19\",\"Jan-20\",\"Feb-20\",\"Mar-20\",\"Apr-20\"]\n","months = [12,31,5]\n","#months = [21]\n","batchindex = 5\n","### Load base geojson grid to export geojson file\n","basejson = []\n","# with open(dir + 'GeoJSON/BaseGrid.geojson', 'r') as file:\n","#     basejson = json.load(file)\n","dt=[]\n","for i, mth in enumerate(months):\n","    dt.append(\"Date : {}, Time : {}\".format(df.iloc[(mth*batchsize+batchindex+embedding_length)*60,0],df.iloc[(mth*batchsize+batchindex+embedding_length)*60,1]))\n","    plotdf[measure+'_rk_'+dt[i]]=0\n","    plotdf[measure+'_ok_'+dt[i]]=0\n","    plotdf[measure+'_ne_'+dt[i]]=0\n","for targetstationid in range(14):    \n","    allstations=np.array(range(14))\n","    otherstationsid=np.delete(allstations,targetstationid).tolist()\n","    import pickle\n","    data=[]\n","    testdata=[]\n","    if batchsize ==1:\n","        for i in range(0,hourdata.shape[0]-embedding_length-batchsize,batchsize):\n","            data.append(Mydata(torch.tensor((np.delete(hourdata[i:i+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32).transpose(0,1),torch.tensor((np.delete(hourdata[i+embedding_length:i+1+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","        for i in range(0,testhourdata.shape[0]-embedding_length-batchsize,batchsize):\n","            testdata.append(Mydata(torch.tensor((np.delete(testhourdata[i:i+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32).transpose(0,1),torch.tensor((np.delete(testhourdata[i+embedding_length:i+1+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","\n","    else:\n","        for i in range(0,hourdata.shape[0]-embedding_length-batchsize,batchsize):\n","            data.append(Mydata(torch.tensor(np.array([(np.delete(hourdata[i+k:i+k+embedding_length,:],targetstationid,axis=1)-mean)/std for k in range(batchsize)]),dtype=torch.float32).transpose(1,2)\n","                        ,torch.tensor((np.delete(hourdata[i+embedding_length:i+batchsize+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","        for i in range(0,testhourdata.shape[0]-embedding_length-batchsize,batchsize):\n","            testdata.append(Mydata(torch.tensor(np.array([(np.delete(testhourdata[i+k:i+k+embedding_length,:],targetstationid,axis=1)-mean)/std for k in range(batchsize)]),dtype=torch.float32).transpose(1,2)\n","                        ,torch.tensor((np.delete(testhourdata[i+embedding_length:i+batchsize+embedding_length,:],targetstationid,axis=1)-mean)/std,dtype=torch.float32)))\n","\n","    with open('TEMmodel_target{}.pkl'.format(targetstationid), 'rb') as f:\n","        mymodel = pickle.load(f)\n","    #mymodel=MyModel(embedding_length=embedding_length,hidden_size=64,output_size=13,batch_size=batchsize)\n","    train_model(mymodel, data, epoch=1000)\n","    with open('TEMmodel_target{}.pkl'.format(targetstationid),'wb') as f:\n","        pickle.dump(mymodel,f)\n","    #mymodel=clf2\n","    \n","\n","    for k in variogram_models:\n","        # print(k)\n","        # model = SVR(C=C, gamma=gamma, kernel=k)\n","        model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n","        # model = LinearRegression(copy_X=True, fit_intercept=False)\n","\n","        scores_mths = []\n","        #\n","        #   continue\n","        \n","        for i, mth in enumerate(months):\n","            \n","            x = np.array(list(zip(wsdf.drop(targetstationid).X, wsdf.drop(targetstationid).Y)))\n","            truevalue = hourdata[mth*batchsize+batchindex+embedding_length,:]\n","            target = mymodel(data[mth].x)[batchindex,:].detach().numpy()*std+mean\n","            # p_train, p_test, x_train, x_test, target_train, target_test = train_test_split(\n","            #     p, x, target, test_size=0.3, random_state=42\n","            # )\n","            print(\"=\" * 40)\n","            m_ok = OrdinaryKriging(x[:,0],x[:,1],target,variogram_model=k,verbose=False)\n","            m_rk = RegressionKriging(regression_model=model, n_closest_points=n, variogram_model=k, verbose=False)\n","\n","            # m_rk.fit(p_train, x_train,target_train)\n","            # score = m_rk.score(p_test, x_test,target_test)\n","            # scores_mths.append(score)\n","\n","            m_rk.fit(p, x, target)\n","            result = m_rk.predict(target_p, target_x)\n","            plotdf.loc[:,measure+'_rk_'+dt[i]].iloc[[targetstationid]] = result[0]-truevalue[targetstationid]\n","            z,sigma=m_ok.execute('points',target_x[0,0],target_x[0,1])\n","            plotdf.loc[:,measure+'_ok_'+dt[i]].iloc[[targetstationid]] = z[0]-truevalue[targetstationid]\n","            plotdf.loc[:,measure+'_ne_'+dt[i]].iloc[[targetstationid]] = truevalue[otherstationsid[np.argmin((wsdf.loc[otherstationsid,'X']-wsdf.loc[targetstationid,'X'])**2+(wsdf.loc[otherstationsid,'Y']-wsdf.loc[targetstationid,'Y'])**2)]]-truevalue[targetstationid]\n","            \n","for i, mth in enumerate(months):\n","    print(dt[i])        \n","    for method in methods:\n","        g = sns.scatterplot(x=\"X\", y=\"Y\",\n","                        hue=measure+method+dt[i],\n","                        palette=\"Spectral_r\",\n","                        data=plotdf.iloc[otherstationsid,:],\n","                        #hue_norm=(-1,1),\n","                        edgecolor=\"black\")\n","        plt.title(\"Temperature prediction error samples, Date : {}, Time : {}\".format(df.iloc[(mth*batchsize+batchindex+embedding_length)*60,0],df.iloc[(mth*batchsize+batchindex+embedding_length)*60,1]))\n","        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n","        plt.show()\n","        # plt.savefig(dir + 'Export/' + measure + mth + '.png', bbox_inches='tight')\n","        #resultdf.to_csv(dir + 'Export/' + measure + mth + '.csv')\n","    error=result-truevalue\n","    display(error)\n","\n","        ### Add properties to geojson and export\n","        # Tmaxes = resultdf['Tmax'].tolist()\n","        # for i in range(len(resultdf.index)):\n","        #     basejson['features'][i]['properties']['Tmax'] = Tmaxes[i]\n","        # with open(dir + 'Export/' + measure + mth + '.geojson', 'w') as outfile:\n","        #     json.dump(basejson, outfile)\n","\n","    # avg = sum(scores_mths) / len(scores_mths)\n","    # scores_all.append([k,C,gamma,n] + scores_mths + [avg] )\n","\n","\n","fig, axes = plt.subplots(clusternum, 1, figsize=(10, 16), sharex=True)\n","for clusterid,ax in enumerate(axes):\n","    sns.boxplot(data = puredata[plotdf['cluster']==clusterid].to_numpy().reshape(-1,24), ax=ax) \n","    ax.set_ylabel(\"\") \n","    ax.set_title(\"Temperature prediction error of stations in cluster {}\".format(clusterid)) \n","    if ax != axes[-1]: \n","        ax.set_xlabel('')\n","    else:\n","        ax.set_xlabel(\"time in the day\")\n","        "]},{"cell_type":"code","execution_count":522,"metadata":{},"outputs":[],"source":["#save the training results\n","'''\n","for targetstationid in range(14):\n","    mymodel = gru(embedding_length=embedding_length,hidden_size=64,output_size=13,batch_size=batchsize)\n","    with open('trainednw\\\\{}grumodel_target{}.pkl'.format(measure,targetstationid),'wb') as f:\n","        pickle.dump(mymodel,f)\n","    mymodel = MyModel(embedding_length=embedding_length,hidden_size=64,output_size=13,batch_size=batchsize)\n","    with open('trainednw\\\\{}lstmmodel_target{}.pkl'.format(measure,targetstationid),'wb') as f:\n","        pickle.dump(mymodel,f)\n","'''"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
