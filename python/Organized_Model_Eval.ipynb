{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "#System and I/O libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "#Numerical and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Visualization libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Machine learning libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#PyTorch libraries for deep learninga\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#pvlib for working with EPW (EnergyPlus Weather) files\n",
    "from pvlib.iotools import read_epw\n",
    "\n",
    "#Geostatistical models - Kriging methods for spatial interpolation\n",
    "from pykrige.rk import RegressionKriging\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "#Serialization library\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataframe from 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define directories and measurement mappings\n",
    "directory = 'C:\\\\Users\\\\zeyuj\\\\OneDrive\\\\Desktop\\\\GNI Repo\\\\24Fa-Microclimate-UWG'\n",
    "\n",
    "#Mappings for measure names and units\n",
    "measurenames = {\"RH0719\": 'RH', \"Tem0719\": 'Temperature'}\n",
    "measureunit = {\"Tem0719\": 'Â°C', \"RH0719\": '%'}\n",
    "\n",
    "#Measurement lists\n",
    "measures = [\"RH0719\", \"Tem0719\"]\n",
    "measure = measures[0]  #Using the first measure\n",
    "testmeasures = [\"RH0819\", \"Tem0819\"]\n",
    "testmeasure = testmeasures[0]  #Using the first test measure\n",
    "\n",
    "#Define the data directory using the updated 'directory' variable\n",
    "datadir = os.path.join(directory, 'data\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "df = pd.read_csv(datadir + measure + '.csv')\n",
    "testdf = pd.read_csv(datadir + testmeasure + '.csv')\n",
    "\n",
    "#Load EPW files (Singapore data)\n",
    "amy, meta = read_epw(datadir + 'SGP_Singapore.486980_IWEC.epw')\n",
    "changi, meta = read_epw(datadir + 'SGP_SINGAPORE-CHANGI-AP_486980S_19.epw')\n",
    "\n",
    "#Map measure keys\n",
    "measurekey = {\"Tem0719\": 'temp_air', \"RH0719\": 'relative_humidity'}\n",
    "mykey = measurekey[measure]\n",
    "\n",
    "#Filter data for July 2019\n",
    "changidata = changi.loc[(changi['year'].astype('str') + changi['month'].astype('str') == '20197'), [mykey]]\n",
    "amydata = amy.loc[(amy['month'].astype('str') == '7'), [mykey]]\n",
    "\n",
    "#Load additional datasets (weather-related)\n",
    "windspeeddf = pd.read_csv(datadir + 'ws0719.csv')\n",
    "solardf = pd.read_csv(datadir + 'sr0719.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Geographic Data Tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset from hugging face since 1 million points is too large for github\n",
    "url = 'https://huggingface.co/datasets/zeyu8800/1MillionGridPoints_TreesAndStreets/resolve/main/1m_GridPoints_distTo_and_zones_3414.csv'\n",
    "griddf = pd.read_csv(url).fillna(0)\n",
    "\n",
    "#Load geographic grid and weather station data, handle missing values\n",
    "wsdf = pd.read_csv(datadir + 'WSPoints_TreesAndStreets_3414.csv').fillna(0)\n",
    "\n",
    "#Data transformations for the grid Dat1 aFrame\n",
    "t = pd.concat([griddf, pd.get_dummies(griddf.HorticultureZone)], axis=1)\n",
    "t = t.replace({\"Temparea\": 1, \"Vegetated Ridge\": 1})\n",
    "griddf = t.drop(['id', 'HorticultureZone'], axis=1)\n",
    "\n",
    "#Set 'distToTree' to 0.0 for areas within 'Vegetated Ridge'\n",
    "griddf['distToTree'].loc[griddf['VegetatedRidge'] == 1] = 0.0\n",
    "\n",
    "#Data transformations for the weather station DataFrame\n",
    "t = pd.concat([wsdf, pd.get_dummies(wsdf.HorticultureZone)], axis=1)\n",
    "t = t.replace({\"Temparea\": 1, \"Vegetated Ridge\": 1, \"Valley\": 1})\n",
    "wsdf = t.drop(['Lat', 'Long', 'HorticultureZone', 'Location', 'Type', 'Description', \n",
    "               'gridarea', 'buildingfootprintarea', 'roadarea', 'patharea', \n",
    "               'walkwayarea', 'courttrackarea', 'carparkarea'], axis=1)\n",
    "wsdf = wsdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data for plotting\n",
    "plotdf = testdf.transpose().iloc[2:16, :]\n",
    "plotdf['X'] = list(wsdf['X'])\n",
    "plotdf['Y'] = list(wsdf['Y'])\n",
    "\n",
    "#Display the resulting DataFrames\n",
    "display(wsdf)\n",
    "display(griddf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot scatterplot of each feature against average Tmax (Mar-19 weather station data)\n",
    "fig, axs = plt.subplots(4, 6, figsize=(18, 9))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(6):\n",
    "        k = i * 6 + j\n",
    "        if k >= len(wsdf.columns):\n",
    "            break\n",
    "        \n",
    "        axs[i][j].scatter(wsdf[wsdf.columns[k]], df.iloc[2:16, 3])\n",
    "        axs[i][j].set_title(wsdf.columns[k])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Plot feature correlations using a heatmap\n",
    "percentfeatures = ['percentroad', 'percentpath', 'percentwalkway', 'percentcourttrack', 'percentcarpark']\n",
    "features = ['terrain', 'distToBuilding', 'distToTree', 'distToWalkway', 'distToRoad', 'distToPath', \n",
    "            'distToCourtTrack', 'distToCarpark']\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(wsdf.corr().loc[features, features], vmin=-1, annot=True, ax=ax)\n",
    "plt.title('Correlation matrix of LULC features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Libraries for LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining features related to percentage of various land uses\n",
    "percentfeatures = ['percentroad', 'percentpath', 'percentwalkway', 'percentcourttrack', 'percentcarpark']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Classes for Neural Network Layers - Geolayer and Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "# Function to return an LSTM model with specified parameters\n",
    "def get_lstm_model(embedding_length, hidden_size, output_size):\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            # Define LSTM layer and output layer\n",
    "            self.hidden_size = hidden_size\n",
    "            self.lstm = nn.LSTM(embedding_length, hidden_size, batch_first=True)\n",
    "            self.label = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        def forward(self, input):\n",
    "            # Dynamically initialize hidden and cell states based on the input batch size\n",
    "            batch_size = input.size(0)\n",
    "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "            \n",
    "            # Forward pass through LSTM\n",
    "            output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "            \n",
    "            # Output from the final hidden state\n",
    "            return self.label(final_hidden_state[-1])\n",
    "    \n",
    "    return LSTMModel()\n",
    "\n",
    "# Function to return a GRU model with specified parameters\n",
    "def get_gru_model(embedding_length, hidden_size, output_size):\n",
    "    class GRUModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(GRUModel, self).__init__()\n",
    "            # Define GRU layer and output layer\n",
    "            self.hidden_size = hidden_size\n",
    "            self.lstm = nn.GRU(embedding_length, hidden_size, batch_first=True)\n",
    "            self.label = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        def forward(self, input):\n",
    "            # Dynamically initialize hidden state based on the input batch size\n",
    "            batch_size = input.size(0)\n",
    "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "            \n",
    "            # Forward pass through GRU\n",
    "            output, final_hidden_state = self.lstm(input, h_0)\n",
    "            \n",
    "            # Output from the final hidden state\n",
    "            return self.label(final_hidden_state[-1])\n",
    "    \n",
    "    return GRUModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, X_train, y_train, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_predictions(puredata, methods, plotdf, base_measure, embedding_length, data_length, batchsize):\n",
    "    \"\"\"\n",
    "    Store the model's predictions in the puredata dictionary with the correct key.\n",
    "\n",
    "    :param puredata: Dictionary to store the predictions\n",
    "    :param methods: List of method suffixes (e.g., '_lstmrk_', '_lstmok_')\n",
    "    :param plotdf: DataFrame of model predictions\n",
    "    :param base_measure: Base measure name (e.g., 'RH0719')\n",
    "    :param embedding_length: Embedding length used for the model\n",
    "    :param data_length: Length of the data\n",
    "    :param batchsize: Size of each batch of data\n",
    "    :return: Updated puredata dictionary\n",
    "    \"\"\"\n",
    "    for method in methods:\n",
    "        # Construct the key as base_measure + method (e.g., 'RH0719_lstmrk_')\n",
    "        full_key = base_measure + method\n",
    "        try:\n",
    "            puredata[full_key] = plotdf.iloc[:, embedding_length:embedding_length + data_length * batchsize].to_numpy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing predictions for {full_key}: {e}\")\n",
    "    return puredata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(datadir, file_name, testdf, wsdf):\n",
    "    \"\"\"\n",
    "    Prepares input data by reading a CSV file, reshaping it, and calculating mean and std.\n",
    "    :param datadir: The directory where the CSV files are located\n",
    "    :param file_name: The name of the CSV file (e.g., 'RH0719') without any method suffix\n",
    "    :param testdf: Test DataFrame\n",
    "    :param wsdf: Weather station DataFrame\n",
    "    :return: hourdata, testhourdata, mean, std, pdhourdata\n",
    "    \"\"\"\n",
    "    # Load the data from the CSV file\n",
    "    npdata = np.array(pd.read_csv(datadir + file_name + '.csv').iloc[:, 2:16])\n",
    "    hourdata = np.sum(npdata.reshape([-1, 60, 14]), axis=1) / 60\n",
    "    testhourdata = np.sum(np.array(testdf.iloc[:, 2:16]).reshape([-1, 60, 14]), axis=1) / 60\n",
    "\n",
    "    # Calculate mean and standard deviation for normalization\n",
    "    mean = hourdata.mean()\n",
    "    std = hourdata.std()\n",
    "\n",
    "    # Create DataFrame for the reshaped hourly data\n",
    "    pdhourdata = pd.DataFrame(hourdata).transpose()\n",
    "    pdhourdata['X'] = list(wsdf['X'])  # Add X coordinates to DataFrame\n",
    "    pdhourdata['Y'] = list(wsdf['Y'])  # Add Y coordinates to DataFrame\n",
    "\n",
    "    return hourdata, testhourdata, mean, std, pdhourdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydata:\n",
    "    def __init__(self, input_tensor, target_tensor):\n",
    "        #Constructor method: Initializes the Mydata object with input data (features) and target data (labels)\n",
    "        #Parameters:\n",
    "        #input_tensor: A tensor containing the features (input data) for the model\n",
    "        #target_tensor: A tensor containing the target (labels) for the corresponding input data\n",
    "        self.x = input_tensor  #Store input data (features) in the object\n",
    "        self.y = target_tensor  #Store target data (labels) in the object    \n",
    "    def __getitem__(self, index):\n",
    "        #Method to retrieve a specific data point from the dataset\n",
    "        #Parameters:\n",
    "        #index: Index of the data point to retrieve\n",
    "        #Returns:\n",
    "        #A tuple containing the input features and the corresponding target value for the given index\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        #Method to get the total number of data points in the dataset\n",
    "        #Returns:\n",
    "        #The length of the input data (i.e., the number of samples in the dataset)\n",
    "        return len(self.x)\n",
    "\n",
    "#Function to prepare training and test data for model training\n",
    "def prepare_data(batchsize, embedding_length, targetstationid, mean, std, hourdata, testhourdata):\n",
    "    data = []  #List to store training data\n",
    "    testdata = []  #List to store test data\n",
    "\n",
    "    #Handle case where batch size is 1\n",
    "    if batchsize == 1:\n",
    "        #Loop over the hourdata to create training batches\n",
    "        for i in range(0, hourdata.shape[0] - embedding_length - batchsize, batchsize):\n",
    "            data.append(Mydata(\n",
    "                torch.tensor((np.delete(hourdata[i:i+embedding_length, :], targetstationid, axis=1) - mean) / std, dtype=torch.float32).transpose(0, 1),\n",
    "                torch.tensor((np.delete(hourdata[i+embedding_length:i+1+embedding_length, :], targetstationid, axis=1) - mean) / std, dtype=torch.float32)\n",
    "            ))\n",
    "        #Loop over the testhourdata to create test batches\n",
    "        for i in range(0, testhourdata.shape[0] - embedding_length - batchsize, batchsize):\n",
    "            testdata.append(Mydata(\n",
    "                torch.tensor((np.delete(testhourdata[i:i+embedding_length, :], targetstationid, axis=1) - mean) / std, dtype=torch.float32).transpose(0, 1),\n",
    "                torch.tensor((np.delete(testhourdata[i+embedding_length:i+1+embedding_length, :], targetstationid, axis=1) - mean) / std, dtype=torch.float32)\n",
    "            ))\n",
    "    else:\n",
    "        #Loop over the data in larger batches\n",
    "        for i in range(0, hourdata.shape[0] - embedding_length - batchsize, batchsize):\n",
    "            data.append(Mydata(\n",
    "                torch.tensor(np.array([(np.delete(hourdata[i+k:i+k+embedding_length, :], targetstationid, axis=1) - mean) / std for k in range(batchsize)]), dtype=torch.float32).transpose(1, 2),\n",
    "                torch.tensor((np.delete(hourdata[i+embedding_length:i+batchsize+embedding_length, :], targetstationid, axis=1) - mean) / std, dtype=torch.float32)\n",
    "            ))\n",
    "        for i in range(0, testhourdata.shape[0] - embedding_length - batchsize, batchsize):\n",
    "            testdata.append(Mydata(\n",
    "                torch.tensor(np.array([(np.delete(testhourdata[i+k:i+k+embedding_length, :], targetstationid, axis=1) - mean) / std for k in range(batchsize)]), dtype=torch.float32).transpose(1, 2),\n",
    "                torch.tensor((np.delete(testhourdata[i+embedding_length:i+batchsize+embedding_length, :], targetstationid, axis=1) - mean) / std, dtype=torch.float32)\n",
    "            ))\n",
    "    \n",
    "    return data, testdata  #Return the prepared training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Matrix and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def calculate_clustering(puredata, measure, nclu=4):\n",
    "    \"\"\"\n",
    "    Calculates the clustering for a given measure using KMeans.\n",
    "    :param puredata: Dictionary containing prediction data\n",
    "    :param measure: The key to use for extracting the measure from puredata\n",
    "    :param nclu: Number of clusters (default is 4)\n",
    "    :return: clusterresult and kmeans\n",
    "    \"\"\"\n",
    "    #Extract error matrix based on the measure and reshape it\n",
    "    errormatrix = np.array(puredata[measure])\n",
    "    errormatrix = errormatrix[:, 8:-16]  # Trim the matrix\n",
    "    tmperror = errormatrix.reshape(-1, 24)  # Reshape to 24-hour segments\n",
    "\n",
    "    #Perform KMeans clustering on the reshaped data\n",
    "    kmeans = KMeans(n_clusters=nclu, random_state=0, n_init=\"auto\").fit(tmperror)\n",
    "    \n",
    "    #Reshape the clustering results and initialize the cluster result matrix\n",
    "    tmpclusterresult = kmeans.labels_.reshape(14, -1)\n",
    "    clusterresult = np.zeros((14, nclu))\n",
    "\n",
    "    #Calculate the cluster membership for each cluster\n",
    "    for i in range(14):\n",
    "        for j in range(nclu):   \n",
    "            clusterresult[i, j] = np.sum(tmpclusterresult[i, :] == j)\n",
    "\n",
    "    #Normalize the cluster result\n",
    "    clusterresult = clusterresult / tmpclusterresult.shape[1]\n",
    "\n",
    "    return clusterresult, kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Time Series Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_clustering(puredata, measure, kmeans, nclu=4, ylabel='(Unit: %)'):\n",
    "    \"\"\"\n",
    "    Plots the time series clustering for the given measure.\n",
    "\n",
    "    :param puredata: Dictionary containing prediction data\n",
    "    :param measure: The key (e.g., 'Tem0719_lstmrk_') to use for extracting the measure from puredata\n",
    "    :param kmeans: KMeans object containing cluster information\n",
    "    :param nclu: Number of clusters (default is 4)\n",
    "    :param ylabel: Label for the y-axis (default is '(Unit: %)')\n",
    "    \"\"\"\n",
    "    #Extract and reshape the time series data for the specified measure\n",
    "    tmparray = np.array(puredata[measure])[:, 8:-16].reshape(-1, 24)\n",
    "\n",
    "    #Set up the color palette for the clusters\n",
    "    colors = sns.color_palette()\n",
    "\n",
    "    #Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    #Plot each individual time series with transparency\n",
    "    for i in range(tmparray.shape[0]):\n",
    "        sns.lineplot(tmparray[i, :], color=colors[kmeans.labels_[i]], alpha=0.08, ax=ax)\n",
    "\n",
    "    #Plot the mean time series for each cluster\n",
    "    for j in range(nclu):\n",
    "        sns.lineplot(tmparray[kmeans.labels_ == j, :].mean(axis=0), color=colors[j], linewidth=2, ax=ax)\n",
    "\n",
    "    #Configure the axis labels and title\n",
    "    ax.set_xticks([0, 6, 12, 18, 24])\n",
    "    ax.set_xlabel('Time of Day')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(f'Time Series Clustering of {measure} Prediction Errors')\n",
    "\n",
    "    #Add the custom legend for clusters\n",
    "    custom_lines = [plt.Line2D([0], [0], color=colors[j], lw=2) for j in range(nclu)]\n",
    "    ax.legend(custom_lines, [f'Cluster {j}' for j in range(nclu)])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding length, batch size, and other necessary parameters\n",
    "embedding_length = 15  # Example value for embedding length\n",
    "batchsize = 16  # Example batch size\n",
    "base_measure = 'RH0719'  # Base measure name (e.g., 'RH0719') without any method suffix\n",
    "\n",
    "datadir = 'C:\\\\Users\\\\zeyuj\\\\OneDrive\\\\Desktop\\\\GNI Repo\\\\24Fa-Microclimate-UWG\\\\data\\\\'\n",
    "\n",
    "nclu = 4  # Number of clusters\n",
    "methods = ['_lstmrk_', '_lstmok_', '_grurk_', '_gruok_']  # Example methods\n",
    "\n",
    "# Step 1: Prepare the input data\n",
    "# Pass the base file name ('RH0719') without the method suffix\n",
    "hourdata, testhourdata, mean, std, pdhourdata = prepare_input_data(datadir, base_measure, testdf, wsdf)\n",
    "\n",
    "# Step 2: Prepare training and test data for model input\n",
    "targetstationid = 0  # Example target station ID, update this based on your data\n",
    "training_data, testing_data = prepare_data(batchsize, embedding_length, targetstationid, mean, std, hourdata, testhourdata)\n",
    "\n",
    "# Step 3: Initialize an empty dictionary for puredata to store predictions\n",
    "puredata = {}\n",
    "\n",
    "# Step 4: Store predictions in puredata using base_measure + method\n",
    "puredata = store_predictions(puredata, methods, pdhourdata, base_measure, embedding_length, len(training_data), batchsize)\n",
    "\n",
    "print(\"Available keys in puredata:\", puredata.keys())\n",
    "\n",
    "# Step 5: Perform clustering on the stored prediction error matrix\n",
    "# Choose the full key for the measure and method combination\n",
    "measure_key = base_measure + '_lstmrk_'  # Example measure key (adjust this based on the method you want to analyze)\n",
    "clusterresult, kmeans = calculate_clustering(puredata, measure_key, nclu=nclu)\n",
    "\n",
    "# Step 6: Add the clustering result back to the wsdf DataFrame\n",
    "wsdf['cluster'] = list(clusterresult)\n",
    "\n",
    "# Step 7: Plot the time series clustering results for the selected measure\n",
    "plot_time_series_clustering(puredata, measure_key, kmeans, nclu=nclu, ylabel='Relative Humidity (%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the measurement type and base file name\n",
    "measure = 'Tem0719'\n",
    "base_measure = measure\n",
    "\n",
    "# Step 1: Prepare the input data using the existing function\n",
    "hourdata, testhourdata, mean, std, pdhourdata = prepare_input_data(datadir, base_measure, testdf, wsdf)\n",
    "\n",
    "# Step 2: Store the reshaped temperature data in the puredata dictionary for clustering and plotting\n",
    "puredata[measure] = pdhourdata.iloc[:, :-2].to_numpy()  # Store only the time series data, excluding 'X' and 'Y' columns\n",
    "\n",
    "# Step 3: Perform KMeans clustering on the stored prediction data\n",
    "clusterresult, kmeans = calculate_clustering(puredata, measure, nclu=nclu)\n",
    "\n",
    "# Step 4: Plot the time series clustering using the existing function\n",
    "plot_time_series_clustering(puredata, measure, kmeans, nclu=nclu, ylabel=f'(Unit: {measureunit[measure]})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions created, except for get_lstm and train_model, were used for the error measuring process, so these 2 are the only important ones. The others were created because the people at NUS decided to initialize dataframes by concatenating the file names instead of calling them directly for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing LSTM Model with our CRC EPW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epw import epw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CRC = epw()\n",
    "CRC.read(r\"C:\\Users\\zeyuj\\OneDrive\\Desktop\\GNI Repo\\24Fa-Microclimate-UWG\\data\\CRC.epw\")\n",
    "\n",
    "# Extract DataFrame from EPW object\n",
    "CRC_df = CRC.dataframe\n",
    "CRC_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Dry Bulb Temperature'\n",
    "feature_cols = ['Year', 'Month', 'Day', 'Hour', 'Dry Bulb Temperature',\n",
    "       'Dew Point Temperature', 'Relative Humidity',\n",
    "       'Atmospheric Station Pressure', 'Extraterrestrial Horizontal Radiation',\n",
    "       'Extraterrestrial Direct Normal Radiation',\n",
    "       'Horizontal Infrared Radiation Intensity',\n",
    "       'Global Horizontal Radiation', 'Direct Normal Radiation',\n",
    "       'Diffuse Horizontal Radiation', 'Global Horizontal Illuminance',\n",
    "       'Direct Normal Illuminance', 'Diffuse Horizontal Illuminance',\n",
    "       'Zenith Luminance', 'Wind Direction', 'Wind Speed', 'Total Sky Cover',\n",
    "       'Opaque Sky Cover (used if Horizontal IR Intensity missing)',\n",
    "       'Visibility', 'Ceiling Height', 'Present Weather Observation',\n",
    "       'Present Weather Codes', 'Precipitable Water', 'Aerosol Optical Depth',\n",
    "       'Snow Depth', 'Days Since Last Snowfall', 'Albedo',\n",
    "       'Liquid Precipitation Depth', 'Liquid Precipitation Quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "features = CRC_df[feature_cols].values\n",
    "target = CRC_df[target_col].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize features and target\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaled = scaler_target.fit_transform(target)\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for LSTM\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    in_seq, out_seq = [], []\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        in_seq.append(input_data[i:i+seq_length])\n",
    "        out_seq.append(target_data[i + seq_length])\n",
    "    return np.array(in_seq), np.array(out_seq)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(features_scaled, target_scaled, sequence_length)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tensor.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "embedding_length = X_train_tensor.shape[2]  # Number of features\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "\n",
    "# Get the LSTM model\n",
    "model = get_lstm_model(embedding_length=embedding_length, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, X_train_tensor, y_train_tensor, criterion, optimizer, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Make predictions on test data\n",
    "    train_predict = model(X_test_tensor)\n",
    "    predicted = train_predict.data.numpy()\n",
    "    actual = y_test_tensor.numpy()\n",
    "\n",
    "    # Inverse transform to get original scale\n",
    "    predicted_actual = scaler_target.inverse_transform(predicted)\n",
    "    actual_actual = scaler_target.inverse_transform(actual)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predicted_actual - actual_actual) ** 2))\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "# Plot actual vs. predicted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_actual, label='Actual Data')\n",
    "plt.plot(predicted_actual, label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel(target_col)\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Make predictions\n",
    "    train_predict = model(X_test_tensor)\n",
    "    predicted = train_predict.data.numpy()\n",
    "    actual = y_test_tensor.numpy()\n",
    "\n",
    "    # Inverse transform to get original scale\n",
    "    predicted_actual = scaler_target.inverse_transform(predicted)\n",
    "    actual_actual = scaler_target.inverse_transform(actual)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predicted_actual - actual_actual) ** 2))\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "# Plot actual vs. predicted for range 1000 to 2500\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_actual[1000:1200], label='Actual Data')\n",
    "plt.plot(predicted_actual[1000:1200], label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel(target_col)\n",
    "plt.title('Actual vs Predicted Values (Time Steps 1000 to 2500)')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
